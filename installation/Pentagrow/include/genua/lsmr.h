
/* Copyright (C) 2015 David Eller <david@larosterna.com>
 *
 * Commercial License Usage
 * Licensees holding valid commercial licenses may use this file in accordance
 * with the terms contained in their respective non-exclusive license agreement.
 * For further information contact david@larosterna.com .
 *
 * GNU General Public License Usage
 * Alternatively, this file may be used under the terms of the GNU General
 * Public License version 3.0 as published by the Free Software Foundation and
 * appearing in the file gpl.txt included in the packaging of this file.
 */

#ifndef GENUA_LSMR_H
#define GENUA_LSMR_H

#include "stanfordsolver.h"
#include "csrmatrix.h"

/** LSMR: Iterative solver for least-squares problems.

  The operator 'A' passed to solve() needs to provide the interface
  \verbatim
    void muladd(const DVector<Scalar> &x, DVector<Scalar> &y) const;
    void muladdTransposed(const DVector<Scalar> &x, DVector<Scalar> &y) const;
  \endverbatim
  which compute y = A*x and y = transpose(A)*x.

  \ingroup numerics
  \sa CraigSolver

   X = LSMR(A,B) solves the system of linear equations A*X=B. If the system
   is inconsistent, it solves the least-squares problem min ||b - Ax||_2.
   A is a rectangular matrix of dimension m-by-n, where all cases are
   allowed: m=n, m>n, or m<n. B is a vector of length m.
   The matrix A may be dense or sparse (usually sparse).

   X = LSMR(AFUN,B) takes a function handle AFUN instead of the matrix A.
   AFUN(X,1) takes a vector X and returns A*X. AFUN(X,2) returns A'*X.
   AFUN can be used in all the following syntaxes.

   X = LSMR(A,B,LAMBDA) solves the regularized least-squares problem
      min ||(B) - (   A    )X||
          ||(0)   (LAMBDA*I) ||_2
   where LAMBDA is a scalar.  If LAMBDA is [] or 0, the system is solved
   without regularization.

   X = LSMR(A,B,LAMBDA,ATOL,BTOL) continues iterations until a certain
   backward error estimate is smaller than some quantity depending on
   ATOL and BTOL.  Let RES = B - A*X be the residual vector for the
   current approximate solution X.  If A*X = B seems to be consistent,
   LSMR terminates when NORM(RES) <= ATOL*NORM(A)*NORM(X) + BTOL*NORM(B).
   Otherwise, LSMR terminates when NORM(A'*RES) <= ATOL*NORM(A)*NORM(RES).
   If both tolerances are 1.0e-6 (say), the final NORM(RES) should be
   accurate to about 6 digits. (The final X will usually have fewer
   correct digits, depending on cond(A) and the size of LAMBDA.)
   If ATOL or BTOL is [], a default value of 1.0e-6 will be used.
   Ideally, they should be estimates of the relative error in the
   entries of A and B respectively.  For example, if the entries of A
   have 7 correct digits, set ATOL = 1e-7. This prevents the algorithm
   from doing unnecessary work beyond the uncertainty of the input data.

   X = LSMR(A,B,LAMBDA,ATOL,BTOL,CONLIM) terminates if an estimate
   of cond(A) exceeds CONLIM. For compatible systems Ax = b,
   conlim could be as large as 1.0e+12 (say).  For least-squares problems,
   conlim should be less than 1.0e+8. If CONLIM is [], the default value
   is CONLIM = 1e+8. Maximum precision can be obtained by setting
   ATOL = BTOL = CONLIM = 0, but the number of iterations may then be
   excessive.

   X = LSMR(A,B,LAMBDA,ATOL,BTOL,CONLIM,ITNLIM) terminates if the
   number of iterations reaches ITNLIM.  The default is ITNLIM = min(m,n).
   For ill-conditioned systems, a larger value of ITNLIM may be needed.

   X = LSMR(A,B,LAMBDA,ATOL,BTOL,CONLIM,ITNLIM,LOCALSIZE) runs LSMR
   with reorthogonalization on the last LOCALSIZE v_k's (v-vectors
   generated by the Golub-Kahan bidiagonalization). LOCALSIZE = 0 or []
   runs LSMR without reorthogonalization. LOCALSIZE = Inf specifies
   full reorthogonalization of the v_k's.  Reorthogonalizing only u_k or
   both u_k and v_k are not an option here, because reorthogonalizing all
   v_k's makes the u_k's close to orthogonal. Details are given in the
   submitted SIAM paper.

   X = LSMR(A,B,LAMBDA,ATOL,BTOL,CONLIM,ITNLIM,LOCALSIZE,SHOW) prints an
   iteration log if SHOW=true. The default value is SHOW=false.

   [X,ISTOP] = LSMR(A,B,...) gives the reason for termination.
      ISTOP  = 0 means X=0 is a solution.
             = 1 means X is an approximate solution to A*X = B,
                 according to ATOL and BTOL.
             = 2 means X approximately solves the least-squares problem
                 according to ATOL.
             = 3 means COND(A) seems to be greater than CONLIM.
             = 4 is the same as 1 with ATOL = BTOL = EPS.
             = 5 is the same as 2 with ATOL = EPS.
             = 6 is the same as 3 with CONLIM = 1/EPS.
             = 7 means ITN reached ITNLIM before the other stopping
                 conditions were satisfied.

   [X,ISTOP,ITN] = LSMR(A,B,...) gives ITN = the number of LSMR iterations.

   [X,ISTOP,ITN,NORMR] = LSMR(A,B,...) gives an estimate of the residual
   norm: NORMR = norm(B-A*X).

   [X,ISTOP,ITN,NORMR,NORMAR] = LSMR(A,B,...) gives an estimate of the
   residual for the normal equation: NORMAR = NORM(A'*(B-A*X)).

   [X,ISTOP,ITN,NORMR,NORMAR,NORMA] = LSMR(A,B,...) gives an estimate of
   the Frobenius norm of A.

   [X,ISTOP,ITN,NORMR,NORMAR,NORMA,CONDA] = LSMR(A,B,...) gives an estimate
   of the condition number of A.

   [X,ISTOP,ITN,NORMR,NORMAR,NORMA,CONDA,NORMX] = LSMR(A,B,...) gives an
   estimate of NORM(X).

   LSMR uses an iterative method requiring matrix-vector products A*v
   and A'*u.  For further information, see
      D. C.-L. Fong and M. A. Saunders,
      LSMR: An iterative algorithm for sparse least-squares problems,
      SIAM J. Sci. Comput., submitted 1 June 2010.
      See http://www.stanford.edu/~clfong/lsmr.html.

 08 Dec 2009: First release version of LSMR.
 09 Apr 2010: Updated documentation and default parameters.
 14 Apr 2010: Updated documentation.
 03 Jun 2010: LSMR with local and/or full reorthogonalization of v_k.
 10 Mar 2011: Bug fix in reorthgonalization. (suggested by David Gleich)

 David Chin-lung Fong            clfong@stanford.edu
 Institute for Computational and Mathematical Engineering
 Stanford University

 Michael Saunders                saunders@stanford.edu
 Systems Optimization Laboratory
 Dept of MS&E, Stanford University.

*/
template <typename Scalar> class LsmrSolver : public SolIterativeSolver
{
public:

  using SolIterativeSolver::ExitCode;

  /// set the number of reorthogonalization vectors
  void reortho(size_t n) { m_vcols = n; }

  /// minimize |Ax - b|^2 + lambda |x|^2
  template <class Operator>
  ExitCode solve(const Operator &A, const DVector<Scalar> &b,
                 DVector<Scalar> &x, Scalar lambda = 0)
  {
    const size_t m = A.nrows();
    const size_t n = A.ncols();
    const size_t minDim = std::min(m, n);

    assert(b.size() >= m);
    DVector<Scalar> u(b);
    Scalar beta = norm(u);
    if (beta > 0)
      u /= beta;

    DVector<Scalar> v(n);
    A.muladdTransposed(u, v);

    Scalar alpha = norm(v);
    if (alpha > 0)
      v /= alpha;

    // Initialization for local reorthogonalization.
    DMatrix<Scalar> localV(n, std::min(m_vcols, minDim));
    size_t colpointer = 0;

    Scalar zetabar(alpha * beta), alphabar(alpha);
    Scalar rho(1), rhobar(1), cbar(1), sbar(0);

    Scalar betadd(beta), betad(0), rhodold(1), tautildeold(0);
    Scalar thetatilde(0), zeta(0), d(0);
    Scalar normA2( sq(alpha) ), maxrbar(0), minrbar(1e+100);
    Scalar normb(beta), normr(beta);
    Scalar ctol(0);
    if (m_conlim > 0)
      ctol = Scalar(1) / m_conlim;

    Scalar normAr = alpha * beta;
    if (normAr == 0)
      return SolutionIsZero;

    DVector<Scalar> h(v);
    DVector<Scalar> hbar(n);
    x.resize(n);

    const size_t itnlim = (m_maxiter <= 0) ? minDim : m_maxiter;
    for (size_t itn = 0; itn < itnlim; ++itn) {

      // Perform the next step of the bidiagonalization to obtain the
      // next beta, u, alpha, v.  These satisfy the relations
      //      beta*u  =  A*v  - alpha*u,
      //      alpha*v  =  A'*u - beta*v.
      A.muladd(v, u, -alpha);
      beta = norm(u);
      if (beta > 0) {
        u /= beta;

        colpointer = enqueue(localV, v, colpointer);
        v *= -beta;
        A.muladdTransposed(u, v);
        orthogonalize(localV, v, colpointer);

        alpha = norm(v);
        if (alpha > 0)
          v /= alpha;
      }

      // At this point, beta = beta_{k+1}, alpha = alpha_{k+1}.

      // Construct rotation Qhat_{k,2k+1}.
      Scalar alphahat = std::sqrt(sq(alphabar) + sq(lambda));
      Scalar chat = alphabar / alphahat;
      Scalar shat = lambda / alphahat;

      // Use a plane rotation (Q_i) to turn B_i to R_i.
      Scalar rhoold = rho;
      rho = std::sqrt(sq(alphahat) + sq(beta));
      Scalar c = alphahat / rho;
      Scalar s = beta / rho;
      Scalar thetanew = s * alpha;
      alphabar = c * alpha;

      // Use a plane rotation (Qbar_i) to turn R_i^T to R_i^bar.
      Scalar rhobarold = rhobar;
      Scalar zetaold = zeta;
      Scalar thetabar = sbar * rho;
      Scalar rhotemp = cbar * rho;
      rhobar = std::sqrt(sq(cbar * rho) + sq(thetanew));
      cbar = cbar * rho / rhobar;
      sbar = thetanew / rhobar;
      zeta = cbar * zetabar;
      zetabar = -sbar * zetabar;

      // Update h, h_hat, x.
      Scalar t3 = (thetabar * rho / (rhoold * rhobarold));
      Scalar t4 = (zeta / (rho * rhobar));

#pragma omp simd
      for (size_t i=0; i<n; ++i) {
        hbar[i] = h[i] - t3*hbar[i];
        x[i] +=  t4 * hbar[i];
        h[i] = v[i] - (thetanew / rho) * h[i];
      }

      // Estimate of ||r||.

      // Apply rotation Qhat_{k,2k+1}.
      Scalar betaacute = chat * betadd;
      Scalar betacheck = -shat * betadd;

      // Apply rotation Q_{k,k+1}.
      Scalar betahat = c * betaacute;
      Scalar betadd = -s * betaacute;

      // Apply rotation Qtilde_{k-1}.
      // betad = betad_{k-1} here.

      Scalar thetatildeold = thetatilde;
      Scalar rhotildeold = std::sqrt(sq(rhodold) + sq(thetabar));
      Scalar ctildeold = rhodold / rhotildeold;
      Scalar stildeold = thetabar / rhotildeold;
      Scalar thetatilde = stildeold * rhobar;
      Scalar rhodold = ctildeold * rhobar;
      betad = -stildeold * betad + ctildeold * betahat;

      // betad   = betad_k here.
      // rhodold = rhod_k  here.

      tautildeold = (zetaold - thetatildeold * tautildeold) / rhotildeold;
      Scalar taud = (zeta - thetatilde * tautildeold) / rhodold;
      d += sq(betacheck);
      normr = std::sqrt(d + sq(betad - taud) + sq(betadd));

      // Estimate ||A||.
      normA2 += sq(beta);
      Scalar normA = std::sqrt(normA2);
      normA2 += sq(alpha);

      // Estimate cond(A).
      maxrbar = std::max(maxrbar, rhobarold);
      if (itn > 0)
        minrbar = std::min(minrbar, rhobarold);
      Scalar condA = std::max(maxrbar, rhotemp) / std::min(minrbar, rhotemp);

      // Test for convergence.
      normAr = std::fabs(zetabar);
      Scalar normx = norm(x);

      // Now use these norms to estimate certain other quantities,
      // some of which will be small near a solution.
      Scalar test1 = normr / normb;
      Scalar test2 = normAr / (normA * normr);
      Scalar test3 = Scalar(1) / condA;
      Scalar t1 = test1 / (1 + normA * normx / normb);
      Scalar rtol = m_btol + m_atol * normA * normx / normb;

      if (verbose())
        std::clog << '[' << itn << "] LSMR |r|: " << normr
                  << " t1: " << t1 << " cond(A): " << condA << std::endl;

      if (1 + test3 <= 1)
        return ConLimTooLarge;
      else if (1 + test2 <= 1)
        return MachinePrecisionSolution;
      else if (1 + t1 <= 1)
        return MachinePrecisionResidual;
      else if (test3 <= ctol)
        return ConLimExceeded;
      else if (test2 <= m_atol)
        return SolutionToleranceAchieved;
      else if (test1 <= rtol)
        return ResidualToleranceAchieved;
    }

    return IterationLimit;
  }

private:

  size_t enqueue(DMatrix<Scalar> &basis, const DVector<Scalar> &v, size_t ptr) const
  {
    basis.assignColumn(ptr, v);
    return (ptr + 1) % basis.ncols();
  }

  void orthogonalize(const DMatrix<Scalar> &basis, DVector<Scalar> &v,
                     size_t ptr) const
  {
    const size_t m = basis.nrows();
    for (size_t i = 0; i < ptr; ++i) {
      Scalar dp(0);
#pragma omp simd reduction(+:dp)
      for (size_t j = 0; j < m; ++j)
        dp += basis(j, i) * v[j];
#pragma omp simd
      for (size_t j = 0; j < m; ++j)
        v[j] -= dp * basis(j, i);
    }
  }

private:

  /// maximum dimension of subspace V
  size_t m_vcols = 16;
};

#endif  // LSMR_H
